## ニューラルネットワーク更新のために
ニューラルネットワークを改善したい
  ↓
重みデータを更新したい(損失関数の値が小さくなるように)
  ↓
更新は勾配方向に行う
  ↓
勾配を出すには、微分を出す必要がある
## 用語集

# 損失関数
重みの出力結果、訓練データ（答えデータ）を入力として、損失度合いを数値で出すもの。
大きいほど、損失（食い違い）が大きい
これらがよく損失関数に使われる
・2乘和誤差
・交差エントロピー誤差

#### 公式
・関数の和の微分は、それぞれの関数の微分の和と同じ
{f(x)+g(x)}' = f(x)' + f(x)'

#### 勾配(法)
全ての偏微分をベクトルとしてまとめたもの。

関数の出力？の最小値をが存在します。その最小値を自動的に求めるアルゴリズムが、勾配法です。
http://www.icrus.org/iida/digirakuda/optimize/grad_1/grad_1.html

傾きのこと。
こうばい（ほう）。
直線の方向を示す数。直線がx軸の正の方向となす角の正接で表される。傾き。方向係数。
勾配ベクトルの意味
（向きの意味）勾配ベクトルの向きは，今いる点からちょっと動いたときに関数の値が一番大きくなる向きである。

スカラー場が最も急激に増加　or　減少　する方向を示す
http://www.agr.nagoya-u.ac.jp/~mechbio/lecture_denjiki_vector2.html

http://s0sem0y.hatenablog.com/entry/2017/05/17/045055

? 下記、確認要
 - 勾配は、各地点において低くなる方向を指す
 - 勾配は各場所において関数の値を最も減らす方向です

### スカラー
向きを持たない値。（向きはベクトル）

#### 微分(法)
微(かす)かな物に分けられた値。
全体から瞬間を取り出されたもの。
「複雑な曲線を，単純な直線に分解する方法」
ある全体の変化量に対して、部分的な変化量を求めること。また、その値。
参考：
  http://naop.jp/topics/topics25.html
  http://qiita.com/43x2/items/aaff93cc0b978223a8f6
  https://datahotel.io/archives/738
  https://sci-pursuit.com/math/differential-1.html
    ある関数（今の場合は、y=x2）の任意の点における傾きを導く式を導関数といい、この導関数を求めることを、一般に微分というのです。
  https://endoyuta.com/2017/01/09/python-%E6%95%B0%E5%80%A4%E5%BE%AE%E5%88%86/
    微分はある瞬間の変化量です。公式は下記です。超絶的に０に近い、というかもはや０という謎の極小値でもって計算することで瞬間変化量を出します。解析的にやるといろんな公式が編み出されているので真の微分が出せますが、コンピュータで公式通りにやろうとすると、謎の極小値を本当にすごい小さい値にすることになるので、誤差がでます。しかしその誤差が非常に小さいので多くの場合問題ありません

例えば、ある関数に渡すパラメータが0から5まで1ずつ増えていく場合に
「3」の時点の微分を出したければ、3 + 0.0001(1e-4) と 3 - 0.0001(1e-4) / 2 * 0.0001(1e-4) という2つのパラメータで微分を求める
また、傾きとは変化の割合である

#### 偏微分
複数の変数からなる関数の微分

#### 極限(lim)
http://dic.nicovideo.jp/a/%E5%BE%AE%E5%88%86

#### Softmax(ソフトマックス)関数
使用する活性化関数によっては負の値が出てきたりと、そのままでは扱いづらいです。そのため、この出力を確率に変換する式
マックス関数（一番大きい成分を 11 にして，それ以外のものは 00 にする関数をこう呼ぶことにする）をソフトにしたという感じです。

#### バイアス
重み計算後に、入力(x)から独立して自動で付与される重みのこと。なんらかの事情により、確定的な+または-を行いたい時に使う　
全体的に（or 範囲グループに）値を偏らせる意味で、広く同じ値を設定するとき使うことが多い

#### Sogmoid(シグモイド)関数
入力値が大きいほど1に近づき、 小さいほど0に近づく。
元の入力の値を殺しすぎない

#### 誤差逆伝搬
最終的な損失関数の値にリンクするノードに誤差を逆にわりふって行き、最終ノードまでの各層に誤差を伝搬(勾配の修正を行うため)

最終ノードより前の損失（誤差）の計算方法
= 最終ノードの誤差に対し、重み（w）を渡した割合をもとめ、渡した側にその割合文の損失を加算していく

#### 畳み込みニューラルネットワーク(CNN)
畳み込み演算を使ったニューラルネットワーク
#### 畳み込み演算
入力データ(n*n次元)に対して、別のフィルター(n*n次元）のデータを積和演算を行う
このフィルターが、普通のニューラルネットワーク（全結合)の重み(バイアス）に当たるものになる

## その他
#### 式の展開(n乘の展開とか)
http://math.005net.com/yoten/tenkai.php
http://memo.sugyan.com/entry/20151124/1448292129
